**Multidimensional scaling (MDS) maps of perceived voice similarity across stimulus sets, listener groups, and solution dimensionality.**

This interactive figure allows readers to explore every individual-differences scaling (INDSCAL) configuration reported in the manuscript.

* **Tab layer 1 – Dimensionality**  
  Selects the dimensionality of the shared perceptual space determined by MDS:
    * **2D**: 2-dimensional solution  
    * **3D**: 3-dimensional solution

* **Tab layer 2 – Stimulus set**  
  Selects the language composition of the stimuli submitted to MDS:
    * **All stimuli**: full stimulus space (Cantonese + English)
    * **Cantonese stimuli**: stimuli consisting of two Cantonese utterances  
    * **English stimuli**: stimuli consisting of two English utterances  
    * **Mixed-language stimuli**: stimuli consisting of one Cantonese and one English utterance

* **Tab layer 3 – Listener group**  
  Selects the listener group whose similarity ratings enter MDS:
    * **All listeners** (pooled across groups)  
    * **Cantonese-English listeners**
    * **English listeners** with no Cantonese proficiency

Each panel visualises individual **speaker tokens** (colour-coded by *speaker identity*) in the shared perceptual space.  
**Marker shape** indicates the language spoken:  
    `●` circle = *Cantonese*
    `■` square = *English*
    `◆` diamond = *Mixed*

Labels concatenate *speaker ID* and *stimulus language*  
(e.g., `VF21B_Eng`; mixed tokens appear as `VF21B_Mixed`).  
Speaker IDs follow the *Speech in Cantonese and English* (SpiCE; Johnson et al., 2020) corpus, which provides the source utterances.

*Hover* over any point to display metadata and exact coordinates.  
*Click* a point to play the waveform(s) used as stimuli.

Axes are centered and scaled to a fixed **–0.8 to 0.8** range for visual comparability; 3D plots can be rotated interactively with the mouse.